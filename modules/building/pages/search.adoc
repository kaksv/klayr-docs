= Setting up docsearch
// Settings:
:hide-uri-scheme:
:idprefix:
:idseparator: -
// Project URLs:
:url-repo: https://gitlab.com/klayrhq/klayr-docs
// External URLs:
:url-algolia: https://www.algolia.com/doc/guides/getting-started/what-is-algolia/
:url-docsearch-scraper-repo: https://github.com/algolia/docsearch-scraper
:url-docsearch-scraper-docs: https://community.algolia.com/docsearch/run-your-own.html
:url-pipenv: https://pipenv.readthedocs.io/en/latest/
:url-doc-susemanager: https://github.com/SUSE/doc-susemanager/wiki/Setup-Algolia-Search-with-Antora
:url-antora-search: https://gitlab.com/antora/antora-ui-default/issues/44

This guide contains information on how to manage the search index for the Klayr documentation site.
It documents where the search index is located and how to update it.

== Overview

The search index for the documentation is hosted by {url-algolia}[Algolia].
The name of the index is `klayr-docs-search`.
The index is populated by the {url-docsearch-scraper-repo}[docsearch scraper] (aka crawler).

The sections below documents the prerequisites for running the docsearch scraper and how to run it to update the index.

== Prerequisites

* git
* Python v3
* {url-pipenv}[pipenv] (to manage a local Python installation and packages)

== Setup

To begin, clone the {url-repo} repository using git and switch to it:

[subs=attributes+]
....
$ git clone {url-repo} &&
cd klayr-docs/build/searchdocs-scraper
....

Remain in the `searchdocs-scraper` folder, and also clone the {url-docsearch-scraper-repo} repository using git:

[subs=attributes+]
 $ git clone {url-docsearch-scraper-repo}

Next, create an `.env` file in the `searchdocs-scraper` directory.
This file is used to define the application ID (`APPLICATION_ID`) and write the API key (`API_KEY`) for the scraper.

.{blank}.env
[source,bash]
----
APPLICATION_ID=E80Q5UHPOS
API_KEY=****************************7ac1
----

Then change the permissions of this file so it cannot be read by others.

 $ chmod 600 .env

NOTE: The API key used in this file is different than the one used for searching.
In the Algolia dashboard, it's labeled as the Admin API Key.

The next step is to use {url-pipenv}[pipenv] to set up a Python environment and install the required packages.

 $ pipenv install --three

 $ pip install python-dotenv

Finally, you'll need the docsearch configuration file.
This configuration file `config.json` is located in the `searchdocs-scraper` directory.

You're now ready to run the scraper.

== Usage

Keep in mind that building the index takes several minutes to complete, as the scraper has to visit every page in the site.

WARNING: The `docsearch docker:run` command will not be available if the APPLICATION_ID and API_KEY are not defined in the `.env` file.

Firstly, ensure you have Docker running on your machine and that you can list images.

 $ docker images

Then, pass the config file to the `docsearch docker:run` command, which launches the provided Docker container (algolia/docsearch-scraper):

 $ pipenv run ./docsearch-scraper/docsearch docker:run ./config.json

Check the console output to verify the index is updated.

TIP: You can remove the Python environment and installed packages by typing `pipenv --rm`.

== See Also

* {url-docsearch-scraper-docs}[Official documentation for the docsearch scraper]
* {url-doc-susemanager}[How to update the search index more info]
* {url-antora-search}[Open issue about adding autocomplete search to Antora default UI]
